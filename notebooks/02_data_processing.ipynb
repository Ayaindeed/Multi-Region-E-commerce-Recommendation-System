{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a33a55a",
   "metadata": {},
   "source": [
    "# Data Processing & Transformation\n",
    "## Brazilian E-commerce Dataset - Phase 2\n",
    "\n",
    "This notebook processes the raw Olist data and prepares it for:\n",
    "- Recommendation engine development\n",
    "- Multi-region deployment simulation\n",
    "- MinIO object storage integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ea611da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Processing data from: ..\\data\\raw\n",
      "‚úÖ Saving processed data to: ..\\data\\processed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set paths\n",
    "data_dir = Path('../data')\n",
    "raw_dir = data_dir / 'raw'\n",
    "processed_dir = data_dir / 'processed'\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Processing data from: {raw_dir}\")\n",
    "print(f\"‚úÖ Saving processed data to: {processed_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0206ce54",
   "metadata": {},
   "source": [
    "## 1. Load and Clean Core Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9c5fa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes:\n",
      "Orders: (99441, 8)\n",
      "Customers: (99441, 5)\n",
      "Order Items: (112650, 7)\n",
      "Products: (32951, 9)\n",
      "Reviews: (99224, 7)\n",
      "Sellers: (3095, 4)\n",
      "Geolocation: (1000163, 5)\n"
     ]
    }
   ],
   "source": [
    "# Load main datasets\n",
    "orders = pd.read_csv(raw_dir / 'olist_orders_dataset.csv')\n",
    "customers = pd.read_csv(raw_dir / 'olist_customers_dataset.csv')\n",
    "order_items = pd.read_csv(raw_dir / 'olist_order_items_dataset.csv')\n",
    "products = pd.read_csv(raw_dir / 'olist_products_dataset.csv')\n",
    "reviews = pd.read_csv(raw_dir / 'olist_order_reviews_dataset.csv')\n",
    "sellers = pd.read_csv(raw_dir / 'olist_sellers_dataset.csv')\n",
    "geolocation = pd.read_csv(raw_dir / 'olist_geolocation_dataset.csv')\n",
    "\n",
    "print(\"Dataset shapes:\")\n",
    "print(f\"Orders: {orders.shape}\")\n",
    "print(f\"Customers: {customers.shape}\")\n",
    "print(f\"Order Items: {order_items.shape}\")\n",
    "print(f\"Products: {products.shape}\")\n",
    "print(f\"Reviews: {reviews.shape}\")\n",
    "print(f\"Sellers: {sellers.shape}\")\n",
    "print(f\"Geolocation: {geolocation.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542f8e0",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edcebe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Orders ===\n",
      "Shape: (99441, 8)\n",
      "Missing values: 4908\n",
      "Duplicate rows: 0\n",
      "Columns with missing values:\n",
      "  order_approved_at: 160 (0.2%)\n",
      "  order_delivered_carrier_date: 1783 (1.8%)\n",
      "  order_delivered_customer_date: 2965 (3.0%)\n",
      "\n",
      "=== Customers ===\n",
      "Shape: (99441, 5)\n",
      "Missing values: 0\n",
      "Duplicate rows: 0\n",
      "\n",
      "=== Order Items ===\n",
      "Shape: (112650, 7)\n",
      "Missing values: 0\n",
      "Columns with missing values:\n",
      "  order_approved_at: 160 (0.2%)\n",
      "  order_delivered_carrier_date: 1783 (1.8%)\n",
      "  order_delivered_customer_date: 2965 (3.0%)\n",
      "\n",
      "=== Customers ===\n",
      "Shape: (99441, 5)\n",
      "Missing values: 0\n",
      "Duplicate rows: 0\n",
      "\n",
      "=== Order Items ===\n",
      "Shape: (112650, 7)\n",
      "Missing values: 0\n",
      "Duplicate rows: 0\n",
      "\n",
      "=== Products ===\n",
      "Shape: (32951, 9)\n",
      "Missing values: 2448\n",
      "Duplicate rows: 0\n",
      "Columns with missing values:\n",
      "  product_category_name: 610 (1.9%)\n",
      "  product_name_lenght: 610 (1.9%)\n",
      "  product_description_lenght: 610 (1.9%)\n",
      "  product_photos_qty: 610 (1.9%)\n",
      "  product_weight_g: 2 (0.0%)\n",
      "  product_length_cm: 2 (0.0%)\n",
      "  product_height_cm: 2 (0.0%)\n",
      "  product_width_cm: 2 (0.0%)\n",
      "\n",
      "=== Reviews ===\n",
      "Shape: (99224, 7)\n",
      "Missing values: 145903\n",
      "Duplicate rows: 0\n",
      "Columns with missing values:\n",
      "Duplicate rows: 0\n",
      "\n",
      "=== Products ===\n",
      "Shape: (32951, 9)\n",
      "Missing values: 2448\n",
      "Duplicate rows: 0\n",
      "Columns with missing values:\n",
      "  product_category_name: 610 (1.9%)\n",
      "  product_name_lenght: 610 (1.9%)\n",
      "  product_description_lenght: 610 (1.9%)\n",
      "  product_photos_qty: 610 (1.9%)\n",
      "  product_weight_g: 2 (0.0%)\n",
      "  product_length_cm: 2 (0.0%)\n",
      "  product_height_cm: 2 (0.0%)\n",
      "  product_width_cm: 2 (0.0%)\n",
      "\n",
      "=== Reviews ===\n",
      "Shape: (99224, 7)\n",
      "Missing values: 145903\n",
      "Duplicate rows: 0\n",
      "Columns with missing values:\n",
      "  review_comment_title: 87656 (88.3%)\n",
      "  review_comment_message: 58247 (58.7%)\n",
      "\n",
      "=== Sellers ===\n",
      "Shape: (3095, 4)\n",
      "Missing values: 0\n",
      "Duplicate rows: 0\n",
      "  review_comment_title: 87656 (88.3%)\n",
      "  review_comment_message: 58247 (58.7%)\n",
      "\n",
      "=== Sellers ===\n",
      "Shape: (3095, 4)\n",
      "Missing values: 0\n",
      "Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check data quality\n",
    "def assess_data_quality(df, name):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "    \n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"Columns with missing values:\")\n",
    "        null_cols = df.isnull().sum()[df.isnull().sum() > 0]\n",
    "        for col, count in null_cols.items():\n",
    "            print(f\"  {col}: {count} ({count/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Assess all datasets\n",
    "assess_data_quality(orders, \"Orders\")\n",
    "assess_data_quality(customers, \"Customers\")\n",
    "assess_data_quality(order_items, \"Order Items\")\n",
    "assess_data_quality(products, \"Products\")\n",
    "assess_data_quality(reviews, \"Reviews\")\n",
    "assess_data_quality(sellers, \"Sellers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e767747",
   "metadata": {},
   "source": [
    "## 3. Create Master Dataset for Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c30e0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating master dataset...\n",
      "After adding customers: (99441, 12)\n",
      "After adding order items: (113425, 18)\n",
      "After adding products: (113425, 26)\n",
      "After adding order items: (113425, 18)\n",
      "After adding products: (113425, 26)\n",
      "After adding sellers: (113425, 29)\n",
      "After adding reviews: (114092, 31)\n",
      "\n",
      "‚úÖ Master dataset created: (114092, 31)\n",
      "Columns: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', 'freight_value', 'product_category_name', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm', 'seller_zip_code_prefix', 'seller_city', 'seller_state', 'review_score', 'review_creation_date']\n",
      "After adding sellers: (113425, 29)\n",
      "After adding reviews: (114092, 31)\n",
      "\n",
      "‚úÖ Master dataset created: (114092, 31)\n",
      "Columns: ['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', 'freight_value', 'product_category_name', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm', 'seller_zip_code_prefix', 'seller_city', 'seller_state', 'review_score', 'review_creation_date']\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive dataset by joining all tables\n",
    "print(\"Creating master dataset...\")\n",
    "\n",
    "# Start with orders and add customer info\n",
    "master_df = orders.merge(customers, on='customer_id', how='left')\n",
    "print(f\"After adding customers: {master_df.shape}\")\n",
    "\n",
    "# Add order items\n",
    "master_df = master_df.merge(order_items, on='order_id', how='left')\n",
    "print(f\"After adding order items: {master_df.shape}\")\n",
    "\n",
    "# Add product info\n",
    "master_df = master_df.merge(products, on='product_id', how='left')\n",
    "print(f\"After adding products: {master_df.shape}\")\n",
    "\n",
    "# Add seller info\n",
    "master_df = master_df.merge(sellers, on='seller_id', how='left')\n",
    "print(f\"After adding sellers: {master_df.shape}\")\n",
    "\n",
    "# Add reviews (optional - not all orders have reviews)\n",
    "master_df = master_df.merge(reviews[['order_id', 'review_score', 'review_creation_date']], \n",
    "                           on='order_id', how='left')\n",
    "print(f\"After adding reviews: {master_df.shape}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Master dataset created: {master_df.shape}\")\n",
    "print(f\"Columns: {list(master_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12f708",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering for Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9c97693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for recommendation system...\n",
      "‚úÖ Features engineered. Rating distribution:\n",
      "implicit_rating\n",
      "1.0    14795\n",
      "2.0     3936\n",
      "3.0     9498\n",
      "3.5       92\n",
      "4.0    22175\n",
      "5.0    63596\n",
      "Name: count, dtype: int64\n",
      "‚úÖ Features engineered. Rating distribution:\n",
      "implicit_rating\n",
      "1.0    14795\n",
      "2.0     3936\n",
      "3.0     9498\n",
      "3.5       92\n",
      "4.0    22175\n",
      "5.0    63596\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Clean and engineer features\n",
    "print(\"Engineering features for recommendation system...\")\n",
    "\n",
    "# Convert dates\n",
    "date_columns = ['order_purchase_timestamp', 'order_approved_at', \n",
    "                'order_delivered_carrier_date', 'order_delivered_customer_date', \n",
    "                'order_estimated_delivery_date', 'review_creation_date']\n",
    "\n",
    "for col in date_columns:\n",
    "    if col in master_df.columns:\n",
    "        master_df[col] = pd.to_datetime(master_df[col], errors='coerce')\n",
    "\n",
    "# Create implicit ratings based on multiple factors\n",
    "def create_implicit_rating(row):\n",
    "    \"\"\"\n",
    "    Create implicit rating (1-5) based on:\n",
    "    - Review score (if available)\n",
    "    - Order status\n",
    "    - Price tier\n",
    "    \"\"\"\n",
    "    # Start with review score if available\n",
    "    if pd.notna(row['review_score']):\n",
    "        return row['review_score']\n",
    "    \n",
    "    # Fall back to order status\n",
    "    if row['order_status'] == 'delivered':\n",
    "        return 4.0  # Assume satisfied if delivered\n",
    "    elif row['order_status'] in ['shipped', 'processing']:\n",
    "        return 3.5\n",
    "    elif row['order_status'] == 'canceled':\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 3.0  # Neutral for unknown status\n",
    "\n",
    "master_df['implicit_rating'] = master_df.apply(create_implicit_rating, axis=1)\n",
    "\n",
    "# Create purchase value tiers\n",
    "master_df['price_tier'] = pd.cut(master_df['price'], \n",
    "                                bins=5, \n",
    "                                labels=['budget', 'low', 'mid', 'high', 'premium'])\n",
    "\n",
    "# Extract date features\n",
    "master_df['purchase_year'] = master_df['order_purchase_timestamp'].dt.year\n",
    "master_df['purchase_month'] = master_df['order_purchase_timestamp'].dt.month\n",
    "master_df['purchase_weekday'] = master_df['order_purchase_timestamp'].dt.dayofweek\n",
    "\n",
    "print(f\"‚úÖ Features engineered. Rating distribution:\")\n",
    "print(master_df['implicit_rating'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5ed21",
   "metadata": {},
   "source": [
    "## 5. Regional Data Splits for Multi-Region Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae3a3558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing geographic distribution...\n",
      "\n",
      "Customer distribution by region:\n",
      "customer_region\n",
      "southeast       78429\n",
      "south           16348\n",
      "northeast       10517\n",
      "central_west     6713\n",
      "north            2085\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total orders: 114092\n",
      "\n",
      "Deployment region distribution:\n",
      "deployment_region\n",
      "region_1    94777\n",
      "region_2    19315\n",
      "Name: count, dtype: int64\n",
      "Region 1: 94777 orders (83.1%)\n",
      "Region 2: 19315 orders (16.9%)\n"
     ]
    }
   ],
   "source": [
    "# Analyze Brazilian regions for multi-region simulation\n",
    "print(\"Analyzing geographic distribution...\")\n",
    "\n",
    "# Map Brazilian states to regions\n",
    "region_mapping = {\n",
    "    # Southeast (Primary region - S√£o Paulo)\n",
    "    'SP': 'southeast', 'RJ': 'southeast', 'MG': 'southeast', 'ES': 'southeast',\n",
    "    \n",
    "    # South (Secondary region)\n",
    "    'PR': 'south', 'SC': 'south', 'RS': 'south',\n",
    "    \n",
    "    # Northeast\n",
    "    'BA': 'northeast', 'PE': 'northeast', 'CE': 'northeast', 'PB': 'northeast',\n",
    "    'RN': 'northeast', 'AL': 'northeast', 'SE': 'northeast', 'PI': 'northeast', 'MA': 'northeast',\n",
    "    \n",
    "    # North\n",
    "    'AM': 'north', 'PA': 'north', 'AC': 'north', 'RO': 'north', \n",
    "    'RR': 'north', 'AP': 'north', 'TO': 'north',\n",
    "    \n",
    "    # Central-West\n",
    "    'GO': 'central_west', 'MT': 'central_west', 'MS': 'central_west', 'DF': 'central_west'\n",
    "}\n",
    "\n",
    "master_df['customer_region'] = master_df['customer_state'].map(region_mapping)\n",
    "master_df['seller_region'] = master_df['seller_state'].map(region_mapping)\n",
    "\n",
    "# Show regional distribution\n",
    "print(\"\\nCustomer distribution by region:\")\n",
    "region_dist = master_df['customer_region'].value_counts()\n",
    "print(region_dist)\n",
    "print(f\"\\nTotal orders: {len(master_df)}\")\n",
    "\n",
    "# For multi-region simulation, we'll use:\n",
    "# - Southeast + South as \"Region 1\" (US-East simulation)\n",
    "# - Northeast + North + Central-West as \"Region 2\" (EU-West simulation)\n",
    "\n",
    "master_df['deployment_region'] = master_df['customer_region'].map({\n",
    "    'southeast': 'region_1',\n",
    "    'south': 'region_1', \n",
    "    'northeast': 'region_2',\n",
    "    'north': 'region_2',\n",
    "    'central_west': 'region_2'\n",
    "})\n",
    "\n",
    "print(\"\\nDeployment region distribution:\")\n",
    "print(master_df['deployment_region'].value_counts())\n",
    "print(f\"Region 1: {master_df['deployment_region'].value_counts()['region_1']} orders ({master_df['deployment_region'].value_counts()['region_1']/len(master_df)*100:.1f}%)\")\n",
    "print(f\"Region 2: {master_df['deployment_region'].value_counts()['region_2']} orders ({master_df['deployment_region'].value_counts()['region_2']/len(master_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8efe13",
   "metadata": {},
   "source": [
    "## 6. Create Recommendation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2189c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating recommendation datasets...\n",
      "Completed orders: 112037\n",
      "Completed orders: 112037\n",
      "\n",
      "User-item matrix shape: (93115, 73)\n",
      "Users (customers): 93115\n",
      "Items (categories): 73\n",
      "Matrix sparsity: 0.986 (98.6% empty)\n",
      "\n",
      "Top 10 most popular product categories:\n",
      "                        purchase_count  avg_rating   avg_price  \\\n",
      "product_category_name                                            \n",
      "cama_mesa_banho                  11227    3.904471   93.356733   \n",
      "beleza_saude                      9638    4.163675  129.757057   \n",
      "esporte_lazer                     8593    4.140929  113.119564   \n",
      "moveis_decoracao                  8325    3.933514   87.151422   \n",
      "informatica_acessorios            7791    3.962328  116.064527   \n",
      "utilidades_domesticas             6898    4.087054   90.763400   \n",
      "relogios_presentes                5940    4.044697  200.356013   \n",
      "telefonia                         4493    3.974182   69.749664   \n",
      "ferramentas_jardim                4321    4.063643  110.323136   \n",
      "automotivo                        4202    4.088529  139.340424   \n",
      "\n",
      "                        popularity_score  \n",
      "product_category_name                     \n",
      "cama_mesa_banho              5615.452236  \n",
      "beleza_saude                 4821.081838  \n",
      "esporte_lazer                4298.570464  \n",
      "moveis_decoracao             4164.466757  \n",
      "informatica_acessorios       3897.481164  \n",
      "utilidades_domesticas        3451.043527  \n",
      "relogios_presentes           2972.022348  \n",
      "telefonia                    2248.487091  \n",
      "ferramentas_jardim           2162.531821  \n",
      "automotivo                   2103.044265  \n",
      "\n",
      "User-item matrix shape: (93115, 73)\n",
      "Users (customers): 93115\n",
      "Items (categories): 73\n",
      "Matrix sparsity: 0.986 (98.6% empty)\n",
      "\n",
      "Top 10 most popular product categories:\n",
      "                        purchase_count  avg_rating   avg_price  \\\n",
      "product_category_name                                            \n",
      "cama_mesa_banho                  11227    3.904471   93.356733   \n",
      "beleza_saude                      9638    4.163675  129.757057   \n",
      "esporte_lazer                     8593    4.140929  113.119564   \n",
      "moveis_decoracao                  8325    3.933514   87.151422   \n",
      "informatica_acessorios            7791    3.962328  116.064527   \n",
      "utilidades_domesticas             6898    4.087054   90.763400   \n",
      "relogios_presentes                5940    4.044697  200.356013   \n",
      "telefonia                         4493    3.974182   69.749664   \n",
      "ferramentas_jardim                4321    4.063643  110.323136   \n",
      "automotivo                        4202    4.088529  139.340424   \n",
      "\n",
      "                        popularity_score  \n",
      "product_category_name                     \n",
      "cama_mesa_banho              5615.452236  \n",
      "beleza_saude                 4821.081838  \n",
      "esporte_lazer                4298.570464  \n",
      "moveis_decoracao             4164.466757  \n",
      "informatica_acessorios       3897.481164  \n",
      "utilidades_domesticas        3451.043527  \n",
      "relogios_presentes           2972.022348  \n",
      "telefonia                    2248.487091  \n",
      "ferramentas_jardim           2162.531821  \n",
      "automotivo                   2103.044265  \n"
     ]
    }
   ],
   "source": [
    "# Create user-item matrix for collaborative filtering\n",
    "print(\"Creating recommendation datasets...\")\n",
    "\n",
    "# Filter for completed orders only\n",
    "completed_orders = master_df[master_df['order_status'].isin(['delivered', 'shipped'])].copy()\n",
    "print(f\"Completed orders: {len(completed_orders)}\")\n",
    "\n",
    "# Create user-item rating matrix\n",
    "user_item_matrix = completed_orders.pivot_table(\n",
    "    index='customer_unique_id',\n",
    "    columns='product_category_name', \n",
    "    values='implicit_rating',\n",
    "    aggfunc='mean',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "print(f\"\\nUser-item matrix shape: {user_item_matrix.shape}\")\n",
    "print(f\"Users (customers): {user_item_matrix.shape[0]}\")\n",
    "print(f\"Items (categories): {user_item_matrix.shape[1]}\")\n",
    "\n",
    "# Calculate sparsity\n",
    "sparsity = (user_item_matrix == 0).sum().sum() / (user_item_matrix.shape[0] * user_item_matrix.shape[1])\n",
    "print(f\"Matrix sparsity: {sparsity:.3f} ({sparsity*100:.1f}% empty)\")\n",
    "\n",
    "# Create product popularity scores\n",
    "product_popularity = completed_orders.groupby('product_category_name').agg({\n",
    "    'order_id': 'count',\n",
    "    'implicit_rating': 'mean',\n",
    "    'price': 'mean'\n",
    "}).rename(columns={\n",
    "    'order_id': 'purchase_count',\n",
    "    'implicit_rating': 'avg_rating',\n",
    "    'price': 'avg_price'\n",
    "})\n",
    "\n",
    "product_popularity['popularity_score'] = (\n",
    "    product_popularity['purchase_count'] * 0.5 + \n",
    "    product_popularity['avg_rating'] * 0.5\n",
    ")\n",
    "\n",
    "print(f\"\\nTop 10 most popular product categories:\")\n",
    "print(product_popularity.sort_values('popularity_score', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd91f144",
   "metadata": {},
   "source": [
    "## 7. Save Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da10647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed datasets...\n",
      "\n",
      "‚úÖ All datasets saved to processed/ directory:\n",
      "  - completed_orders.csv\n",
      "  - master_dataset.csv\n",
      "  - processing_summary.json\n",
      "  - product_popularity.csv\n",
      "  - region_1_orders.csv\n",
      "  - region_2_orders.csv\n",
      "  - user_item_matrix.csv\n",
      "\n",
      "üìä Processing Summary:\n",
      "  Total orders: 114,092\n",
      "  Completed orders: 112,037\n",
      "  Unique customers: 96,096\n",
      "  Product categories: 73\n",
      "  Region 1 orders: 92,997\n",
      "  Region 2 orders: 19,040\n",
      "  User-item matrix: (93115, 73)\n",
      "  Matrix sparsity: 98.6%\n",
      "\n",
      "‚úÖ All datasets saved to processed/ directory:\n",
      "  - completed_orders.csv\n",
      "  - master_dataset.csv\n",
      "  - processing_summary.json\n",
      "  - product_popularity.csv\n",
      "  - region_1_orders.csv\n",
      "  - region_2_orders.csv\n",
      "  - user_item_matrix.csv\n",
      "\n",
      "üìä Processing Summary:\n",
      "  Total orders: 114,092\n",
      "  Completed orders: 112,037\n",
      "  Unique customers: 96,096\n",
      "  Product categories: 73\n",
      "  Region 1 orders: 92,997\n",
      "  Region 2 orders: 19,040\n",
      "  User-item matrix: (93115, 73)\n",
      "  Matrix sparsity: 98.6%\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "# Main datasets\n",
    "master_df.to_csv(processed_dir / 'master_dataset.csv', index=False)\n",
    "completed_orders.to_csv(processed_dir / 'completed_orders.csv', index=False)\n",
    "\n",
    "# Recommendation matrices\n",
    "user_item_matrix.to_csv(processed_dir / 'user_item_matrix.csv')\n",
    "product_popularity.to_csv(processed_dir / 'product_popularity.csv')\n",
    "\n",
    "# Regional splits\n",
    "region_1_data = completed_orders[completed_orders['deployment_region'] == 'region_1']\n",
    "region_2_data = completed_orders[completed_orders['deployment_region'] == 'region_2']\n",
    "\n",
    "region_1_data.to_csv(processed_dir / 'region_1_orders.csv', index=False)\n",
    "region_2_data.to_csv(processed_dir / 'region_2_orders.csv', index=False)\n",
    "\n",
    "# Create summary statistics\n",
    "summary_stats = {\n",
    "    'processing_date': datetime.now().isoformat(),\n",
    "    'total_orders': len(master_df),\n",
    "    'completed_orders': len(completed_orders),\n",
    "    'unique_customers': master_df['customer_unique_id'].nunique(),\n",
    "    'unique_products': master_df['product_id'].nunique(),\n",
    "    'product_categories': master_df['product_category_name'].nunique(),\n",
    "    'date_range': {\n",
    "        'start': master_df['order_purchase_timestamp'].min().isoformat() if pd.notna(master_df['order_purchase_timestamp'].min()) else None,\n",
    "        'end': master_df['order_purchase_timestamp'].max().isoformat() if pd.notna(master_df['order_purchase_timestamp'].max()) else None\n",
    "    },\n",
    "    'regions': {\n",
    "        'region_1': len(region_1_data),\n",
    "        'region_2': len(region_2_data)\n",
    "    },\n",
    "    'user_item_matrix': {\n",
    "        'shape': user_item_matrix.shape,\n",
    "        'sparsity': float(sparsity)\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(processed_dir / 'processing_summary.json', 'w') as f:\n",
    "    json.dump(summary_stats, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\n‚úÖ All datasets saved to processed/ directory:\")\n",
    "for file in processed_dir.glob('*'):\n",
    "    print(f\"  - {file.name}\")\n",
    "\n",
    "print(f\"\\nüìä Processing Summary:\")\n",
    "print(f\"  Total orders: {summary_stats['total_orders']:,}\")\n",
    "print(f\"  Completed orders: {summary_stats['completed_orders']:,}\")\n",
    "print(f\"  Unique customers: {summary_stats['unique_customers']:,}\")\n",
    "print(f\"  Product categories: {summary_stats['product_categories']}\")\n",
    "print(f\"  Region 1 orders: {summary_stats['regions']['region_1']:,}\")\n",
    "print(f\"  Region 2 orders: {summary_stats['regions']['region_2']:,}\")\n",
    "print(f\"  User-item matrix: {summary_stats['user_item_matrix']['shape']}\")\n",
    "print(f\"  Matrix sparsity: {summary_stats['user_item_matrix']['sparsity']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fb67b",
   "metadata": {},
   "source": [
    "## 8. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a44b94fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating processed datasets...\n",
      "\n",
      "=== Validation: Master Dataset ===\n",
      "‚úÖ Shape: (114092, 39)\n",
      "‚úÖ All required columns present\n",
      "üìä Duplicate rows: 141\n",
      "üìä Data types: {dtype('O'): 15, dtype('float64'): 13, dtype('<M8[ns]'): 6, dtype('int32'): 3, dtype('int64'): 1, CategoricalDtype(categories=['budget', 'low', 'mid', 'high', 'premium'], ordered=True, categories_dtype=object): 1}\n",
      "\n",
      "=== Validation: Completed Orders ===\n",
      "‚úÖ Shape: (112037, 39)\n",
      "‚úÖ All required columns present\n",
      "üìä Duplicate rows: 141\n",
      "üìä Data types: {dtype('O'): 15, dtype('float64'): 13, dtype('<M8[ns]'): 6, dtype('int32'): 3, dtype('int64'): 1, CategoricalDtype(categories=['budget', 'low', 'mid', 'high', 'premium'], ordered=True, categories_dtype=object): 1}\n",
      "\n",
      "=== Validation: Completed Orders ===\n",
      "‚úÖ Shape: (112037, 39)\n",
      "‚úÖ All required columns present\n",
      "üìä Duplicate rows: 140\n",
      "üìä Data types: {dtype('O'): 15, dtype('float64'): 13, dtype('<M8[ns]'): 6, dtype('int32'): 3, dtype('int64'): 1, CategoricalDtype(categories=['budget', 'low', 'mid', 'high', 'premium'], ordered=True, categories_dtype=object): 1}\n",
      "\n",
      "=== Validation: User-Item Matrix ===\n",
      "‚úÖ Shape: (93115, 73)\n",
      "üìä Duplicate rows: 91303\n",
      "üìä Data types: {dtype('float64'): 73}\n",
      "\n",
      "‚úÖ Overall validation: PASSED\n",
      "\n",
      "üéâ Data processing completed successfully!\n",
      "üìÅ Processed files ready for MinIO upload and database loading\n",
      "‚è≠Ô∏è  Next step: Run scripts/setup_minio.py\n",
      "üìä Duplicate rows: 140\n",
      "üìä Data types: {dtype('O'): 15, dtype('float64'): 13, dtype('<M8[ns]'): 6, dtype('int32'): 3, dtype('int64'): 1, CategoricalDtype(categories=['budget', 'low', 'mid', 'high', 'premium'], ordered=True, categories_dtype=object): 1}\n",
      "\n",
      "=== Validation: User-Item Matrix ===\n",
      "‚úÖ Shape: (93115, 73)\n",
      "üìä Duplicate rows: 91303\n",
      "üìä Data types: {dtype('float64'): 73}\n",
      "\n",
      "‚úÖ Overall validation: PASSED\n",
      "\n",
      "üéâ Data processing completed successfully!\n",
      "üìÅ Processed files ready for MinIO upload and database loading\n",
      "‚è≠Ô∏è  Next step: Run scripts/setup_minio.py\n"
     ]
    }
   ],
   "source": [
    "# Validate processed data\n",
    "print(\"Validating processed datasets...\")\n",
    "\n",
    "def validate_dataset(df, name, required_columns=None):\n",
    "    print(f\"\\n=== Validation: {name} ===\")\n",
    "    print(f\"‚úÖ Shape: {df.shape}\")\n",
    "    \n",
    "    # Check for required columns\n",
    "    if required_columns:\n",
    "        missing_cols = set(required_columns) - set(df.columns)\n",
    "        if missing_cols:\n",
    "            print(f\"‚ùå Missing columns: {missing_cols}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ All required columns present\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"üìä Duplicate rows: {duplicates}\")\n",
    "    \n",
    "    # Check data types\n",
    "    print(f\"üìä Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "    \n",
    "    return df.shape[0] > 0 and (not required_columns or not missing_cols)\n",
    "\n",
    "# Validate key datasets\n",
    "validations = {\n",
    "    'Master Dataset': validate_dataset(\n",
    "        master_df, \n",
    "        'Master Dataset',\n",
    "        ['customer_unique_id', 'product_id', 'implicit_rating', 'deployment_region']\n",
    "    ),\n",
    "    'Completed Orders': validate_dataset(\n",
    "        completed_orders,\n",
    "        'Completed Orders', \n",
    "        ['customer_unique_id', 'product_category_name', 'implicit_rating']\n",
    "    ),\n",
    "    'User-Item Matrix': validate_dataset(\n",
    "        user_item_matrix,\n",
    "        'User-Item Matrix'\n",
    "    )\n",
    "}\n",
    "\n",
    "all_valid = all(validations.values())\n",
    "print(f\"\\n{'‚úÖ' if all_valid else '‚ùå'} Overall validation: {'PASSED' if all_valid else 'FAILED'}\")\n",
    "\n",
    "if all_valid:\n",
    "    print(\"\\nüéâ Data processing completed successfully!\")\n",
    "    print(\"üìÅ Processed files ready for MinIO upload and database loading\")\n",
    "    print(\"‚è≠Ô∏è  Next step: Run scripts/setup_minio.py\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Some validations failed. Please review the data processing steps.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
